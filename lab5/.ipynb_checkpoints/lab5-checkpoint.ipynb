{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Multi-Layer Perceptron\n",
    "####  How to make a brain and evaluate it good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### [5 points] Business Case Explanation\n",
    "(mostly the same processes as from lab four) Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the task is and what parties would be interested in the results. How well would your prediction algorithm need to perform to be considered useful by interested parties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Our Problem\n",
    "There are literally tens of thousands of movies out there today. While some do great at the box office and bring in a lot of money, others flop making only a fraction compared to the top hits. What if we had a scientific way of accurately predicting how much revenue a movie would generate over its lifetime? Well, through machine learning we believe that we actually can!\n",
    "\n",
    "The dataset we are using is found on <a href=\"https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset\">Kaggle</a>. It consists of 5000+ movies scraped from the review site IMDB. There is quite a bit of data recorded for each movie and so we had a lot to work with to try to predict the next big hit. The data was collected from web scraping IMDB using a python library called \"scrappy\" to collect all of the data below. The features recorded for each movie are: \n",
    "\n",
    "Basic Info:\n",
    "- movie title\n",
    "- color (black and white or color)\t\n",
    "- duration of the movie\n",
    "- director name\n",
    "- gross (total revenue)\n",
    "- genres (a lits of different genres ascribed to the movie)\n",
    "- number of faces in movie poster\n",
    "- language of the movie\n",
    "- country the movie was produced in\n",
    "- content rating (G, PG, PG-13, R, NC-17)\n",
    "- budget\n",
    "- year of release\n",
    "- aspect ratio\n",
    "- name of the 3rd actor\n",
    "- name of the 2nd actor\n",
    "- name of the 1st actor\n",
    "\n",
    "Facebook Info:\n",
    "- number of director facebook likes\n",
    "- number of facebook likes for the whole cast\n",
    "- number of the movie's facebook likes\n",
    "- number of the 3rd actor's facebook likes\n",
    "- number of the 2nd actor's facebook likes\n",
    "- number of the 1st actor's facebook likes\n",
    "\n",
    "IMDB Specific Info:\n",
    "- number of imdb users who rated the movie\n",
    "- number of critical reviews for the movie\n",
    "- number of users who left a review\n",
    "- imdb score\n",
    "- top plot keywords\n",
    "\n",
    "\n",
    "With all of this data collected on so many movies, we hope to be able to use this to build out a multi-layer perceptron  to accurately predict the financial success (measured in categories of gross revenue: low, low-mid, high-mid, and high) of a movie. We think that this could be a useful tool to anyone in the movie industry who is concerned with making a profit on their movie. It could also help a producer understand which of these features are the most important to an accurate prediction, what content rating is most important, how budget affects outcome, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing:\n",
    "\n",
    "Pre-processing of the CSV:\n",
    "- We first removed the imdb link from the csv because we knew we would never need to use that (**Note: this was the only feature removed from the csv**)\n",
    "- We then went through and deleted all of the movies that were made in another country (foriegn films) we did this because we wanted to just look at American films, also because the currency units for those countries (for budget and gross) were in native currency units, not USD, and with changing exchange rates, it's not very easy to compare across countries.\n",
    "- We then went through and converted all 0 values for gross, movie_facebook_likes, and director_facebook_likes to a blank value in the csv (so that it is read in as NaN by pandas), this is so that we cna more easily impute values later. Note: according to the description on the kaggle entry, because of the way the data was scraped, some movies had missing data. The Python scraper just made these values into a 0 instead of NaN.\n",
    "- We then removed all movies with an undefined gross. Being the feature we are trying to predict, we should not be imputing values for gross to train our model. That will basically reduce our model to an imputation algorithm...\n",
    "- We then removed all movies that were made before 1935. We did this because there were only a handful of movies ranging from 1915 to 1935, the way we are classifying budget (described below) would not work with a small sample of movies from that time period. We could have cut this number at a different year (say 1960), but we didn't want to exclude such classics as \"Bambi\" or \"Gone With the Wind\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing of the Data:\n",
    "- After the above steps, we made more edits to the data using pandas. First, we removed features that we thought would be un-useful to our prediction algorithm. We removed all features concerning facebook likes. We did this because a significant portion of the movies in the training set debuted before facebook was invented and widely adopted. While some of these movies have received retroactive \"likes\" on facebook, only the most famous classics received a substantial amount of retraoctive \"likes\". Most lesser known films received very low amounts of \"likes\" (presumably because modern movie watchers don't really care to search for lesser known movies on facebook, or because the movie doesn't have a facebook). For this reason we decided to remove movie_facebook_likes\n",
    "- Likewise, we removed the other \"likes\" for the same reasons as above. For example, the esteemed director George Lucas has a total of 0 \"likes\" between all of his films. This feature obviously would not help us predict the profitability of movies.\n",
    "- We also removed irrelevant information such as aspect_ratio, language, and country. Because we deleted all foreign films the country will always be USA. A simple filter of the data reveals that there are no more than 20 movies made in the US that use a language other than English, therefore there is not enough data to use language as training feature. However, we did not delete the movies in a different language, because most of them were famous films such as *Letters from Iwo Jima* and *The Kite Runner*. We still count them as a valuable part of the dataset, just don't find the language of particular value. Lastly, we removed aspect_ratio because that seems to be unimportant for predicting the success of a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Define and Prepare Class Variables\n",
    " (mostly the same processes as from lab one) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3222 entries, 0 to 3221\n",
      "Data columns (total 12 columns):\n",
      "director_name             3222 non-null object\n",
      "num_critic_for_reviews    3219 non-null float64\n",
      "duration                  3221 non-null float64\n",
      "gross                     3222 non-null int64\n",
      "actor_1_name              3220 non-null object\n",
      "num_voted_users           3222 non-null int64\n",
      "facenumber_in_poster      3216 non-null float64\n",
      "num_user_for_reviews      3221 non-null float64\n",
      "content_rating            3196 non-null object\n",
      "budget                    3062 non-null float64\n",
      "title_year                3222 non-null int64\n",
      "imdb_score                3222 non-null float64\n",
      "dtypes: float64(6), int64(3), object(3)\n",
      "memory usage: 302.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Take the dataframe and adjust for inflation and then use the df_to_csv function to export to csv \n",
    "# and then export to csv and then delete code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"inflation_corrected_dataset.csv\")\n",
    "for x in ['movie_facebook_likes', 'director_facebook_likes', 'actor_2_facebook_likes', \n",
    "          'actor_1_facebook_likes','actor_3_facebook_likes', 'cast_total_facebook_likes',\n",
    "          'aspect_ratio', 'language', 'country', 'plot_keywords', 'actor_3_name', 'actor_2_name', 'movie_title', 'genres', 'color']:\n",
    "    if x in df:\n",
    "        del df[x]\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tamper with the groupings to improve imputations? How do we improve how many values get imputed?\n",
    "df_grouped = df.groupby(by=['director_name'])\n",
    "# director_name adds about 50 rows (imputes about 50 rows and then deletes about 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3127 entries, 0 to 3220\n",
      "Data columns (total 12 columns):\n",
      "num_critic_for_reviews    3127 non-null float64\n",
      "duration                  3127 non-null float64\n",
      "gross                     3127 non-null int64\n",
      "num_voted_users           3127 non-null int64\n",
      "facenumber_in_poster      3127 non-null float64\n",
      "num_user_for_reviews      3127 non-null float64\n",
      "budget                    3127 non-null float64\n",
      "title_year                3127 non-null int64\n",
      "imdb_score                3127 non-null float64\n",
      "actor_1_name              3127 non-null object\n",
      "content_rating            3127 non-null object\n",
      "director_name             3127 non-null object\n",
      "dtypes: float64(6), int64(3), object(3)\n",
      "memory usage: 317.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n",
    "col_deleted = list( set(df.columns) - set(df_imputed.columns)) #in case the median op deleted columns\n",
    "df_imputed[col_deleted] = df[col_deleted]\n",
    "\n",
    "# drop rows that still have missing values after imputation\n",
    "df_imputed.dropna(inplace=True)\n",
    "print(df_imputed.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Data\n",
    "Below we scale the data using the methods shown so as to not adversely affect the gamma value. We scaled down all of the numerical values to be within -1 and 1. We also one-hot encode the content rating. We forego encoding the director name or actor names because they proved to make our matrix way too large to run computations on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:  (3127, 12)\n",
      "(3127, 18)\n",
      "Wall time: 7.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#scaling budgets!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "\n",
    "\n",
    "budget = df_imputed['budget'].values.reshape(-1, 1)\n",
    "df_imputed.reset_index(drop=True, inplace=True)\n",
    "print(\"df: \",df_imputed.shape)\n",
    "\n",
    "append_list = [df_imputed]\n",
    "\n",
    "budget_scaler = StandardScaler()\n",
    "append_list.append(pd.DataFrame(budget_scaler.fit_transform(budget), columns=['scaled_budget']))\n",
    "\n",
    "gross = df_imputed['gross'].values.reshape(-1,1)\n",
    "gross_scaler = StandardScaler()\n",
    "append_list.append(pd.DataFrame(gross_scaler.fit_transform(gross), columns=['scaled_gross']))\n",
    "\n",
    "critics = df_imputed['num_critic_for_reviews'].values.reshape(-1, 1)\n",
    "critic_scaler = StandardScaler()\n",
    "append_list.append(pd.DataFrame(critic_scaler.fit_transform(critics), columns=['scaled_critics']))\n",
    "# print(df)\n",
    "\n",
    "duration = df_imputed['duration'].values.reshape(-1, 1)\n",
    "duration_scaler = StandardScaler()\n",
    "append_list.append(pd.DataFrame(duration_scaler.fit_transform(duration), columns=['scaled_duration']))\n",
    "         \n",
    "num_voted_users = df_imputed['num_voted_users'].values.reshape(-1, 1)\n",
    "voted_scaler = StandardScaler()\n",
    "append_list.append(pd.DataFrame(voted_scaler.fit_transform(num_voted_users), columns=['scaled_voted_users']))\n",
    "\n",
    "num_user_for_reviews = df_imputed['num_user_for_reviews'].values.reshape(-1,1)\n",
    "user_reviews_scaler = StandardScaler()\n",
    "append_list.append(pd.DataFrame(user_reviews_scaler.fit_transform(num_user_for_reviews), columns=['scaled_user_reviews']))\n",
    "\n",
    "df = pd.concat(append_list, axis=1)\n",
    "\n",
    "#one-hot encode\n",
    "hot_content = pd.get_dummies(df_imputed.content_rating, prefix='contentRating')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting the gross into categories\n",
    "Below we cut the adjusted, scaled, gross into 4 main categories: low, low-mid, high-mid, and high. We did this because otherwise the model would not be able to produce raw gross accurately. We also used the \"qcut\" function to evenly distribute the classes among the classifications, because when we did a normal cut method most of the classes would fall in the lowest category and throw off our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "spacing = np.linspace(0, max(df['gross']), 100)\n",
    "labels = []\n",
    "\n",
    "labels = [\"low\", \"low-mid\", \"high-mid\", \"high\"]\n",
    "df['gross_group'] = pd.qcut(df['gross'], 4, labels=labels)\n",
    "\n",
    "\n",
    "rating_group = df['gross_group'].values\n",
    "rating_encoder = LabelEncoder()\n",
    "rating_df = pd.DataFrame(rating_encoder.fit_transform(rating_group), columns=['encoded_gross']).astype(str)\n",
    "df = pd.concat([df, rating_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it took too much time to adjust for inflation, we just did that and then exported it to a csv and then slapped that momma up with some secret sauce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### [15 points] Choose and explain evaluation metrics\n",
    "Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s generalization performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "- Having false positives is TERRIBLE because then somebody will be SCREWED because they will make the movie thinking they will make money but they don't\n",
    "- Having false negatives is not as bad, because it means the director will pass on the movie, menaing they will miss out on the opportunity but won't make money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1  10  70 100]\n",
      " [  3  -1  10  70]\n",
      " [  7   3  -1  10]\n",
      " [ 10   7   3  -1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cost_matrix = np.array([-1,10,70,100,3,-1,10,70,7,3,-1,10,10,7,3,-1]) #give a reason for why these numbers chosen\n",
    "cost_matrix = cost_matrix.reshape(4,4)\n",
    "print(cost_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Method for dividing up training/testing data\n",
    "Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      facenumber_in_poster  title_year  imdb_score  scaled_budget  \\\n",
      "0                      1.0        1947         7.7      -0.493563   \n",
      "1                      0.0        1948         7.1      -0.261700   \n",
      "2                      0.0        1950         7.0      -0.242618   \n",
      "3                      0.0        1952         6.7      -0.285389   \n",
      "4                      1.0        1953         6.8      -0.113499   \n",
      "5                      0.0        1953         6.7      -0.961273   \n",
      "6                      2.0        1954         8.2      -0.839174   \n",
      "7                      2.0        1959         8.3      -0.529746   \n",
      "8                      2.0        1960         8.5      -0.868898   \n",
      "9                      0.0        1961         7.6      -0.050409   \n",
      "10                     0.0        1963         7.6       0.457584   \n",
      "11                     1.0        1964         7.9       1.592118   \n",
      "12                     2.0        1964         7.8      -0.084126   \n",
      "13                     1.0        1964         8.1      -0.974059   \n",
      "14                     6.0        1965         6.6       2.016446   \n",
      "15                     2.0        1965         8.0       0.659747   \n",
      "16                     3.0        1965         8.0       0.237662   \n",
      "17                     2.0        1965         6.8      -0.425613   \n",
      "18                     1.0        1969         8.1      -0.206019   \n",
      "19                     2.0        1970         6.2       2.111651   \n",
      "20                     8.0        1970         6.2      -0.886478   \n",
      "21                     0.0        1970         8.1      -0.923799   \n",
      "22                     0.0        1971         8.0       0.065054   \n",
      "23                     1.0        1971         5.5      -0.939358   \n",
      "24                     1.0        1971         6.7      -0.995251   \n",
      "25                     1.0        1972         9.2      -0.311807   \n",
      "26                     2.0        1972         6.1      -0.997297   \n",
      "27                     0.0        1973         8.0      -0.115063   \n",
      "28                     0.0        1973         8.3      -0.391119   \n",
      "29                     1.0        1973         7.5      -0.912643   \n",
      "30                     0.0        1973         7.4      -0.943230   \n",
      "31                     1.0        1974         9.0       0.311273   \n",
      "32                     0.0        1974         8.0      -0.716349   \n",
      "33                     0.0        1974         7.8      -0.736498   \n",
      "34                     0.0        1974         6.1      -0.948067   \n",
      "35                     0.0        1974         7.5      -0.990025   \n",
      "36                     0.0        1974         7.5      -0.990025   \n",
      "37                     0.0        1975         8.0      -0.277278   \n",
      "38                     0.0        1975         8.7      -0.601802   \n",
      "39                     1.0        1976         6.8      -0.237883   \n",
      "40                     0.0        1976         8.1      -0.917315   \n",
      "41                    14.0        1977         7.4       1.089772   \n",
      "42                     0.0        1977         7.7       0.559757   \n",
      "43                     1.0        1977         8.7      -0.114966   \n",
      "44                     0.0        1977         8.1      -0.677178   \n",
      "45                     0.0        1977         5.9      -0.958283   \n",
      "46                     0.0        1978         7.3       3.137469   \n",
      "47                     0.0        1978         5.2       0.806320   \n",
      "48                     0.0        1978         5.7       0.505526   \n",
      "49                     2.0        1978         7.2      -0.547251   \n",
      "50                     6.0        1978         7.6      -0.772846   \n",
      "51                     0.0        1978         7.9      -0.975882   \n",
      "52                     0.0        1978         7.9      -0.975882   \n",
      "53                     0.0        1978         7.9      -0.975882   \n",
      "54                     0.0        1978         8.2       0.401293   \n",
      "55                     2.0        1979         6.4       1.410456   \n",
      "56                     1.0        1979         8.5       1.169566   \n",
      "57                     0.0        1979         6.9      -0.413423   \n",
      "58                     0.0        1979         7.6      -0.447836   \n",
      "59                     4.0        1980         6.8       2.265562   \n",
      "60                     0.0        1980         6.8       1.661117   \n",
      "61                     1.0        1980         7.9       0.633560   \n",
      "62                     0.0        1980         4.5       0.210449   \n",
      "63                     0.0        1980         8.8       0.089560   \n",
      "64                     1.0        1980         8.3       0.089560   \n",
      "65                     1.0        1980         6.1      -0.393996   \n",
      "66                     0.0        1980         7.1      -0.605552   \n",
      "67                     3.0        1980         7.8      -0.635774   \n",
      "68                     3.0        1980         7.4      -0.635774   \n",
      "69                     0.0        1980         7.8      -0.786885   \n",
      "70                     0.0        1980         6.8      -0.937997   \n",
      "71                     0.0        1980         6.8      -0.937997   \n",
      "72                     0.0        1980         7.5       0.084693   \n",
      "73                     0.0        1981         5.4       0.082994   \n",
      "74                     1.0        1981         8.5      -0.025149   \n",
      "75                     0.0        1981         5.0      -0.025149   \n",
      "76                     0.0        1981         6.7      -0.025149   \n",
      "77                     0.0        1981         6.4      -0.079221   \n",
      "78                     0.0        1981         5.2      -0.241436   \n",
      "79                     1.0        1981         6.9      -0.457723   \n",
      "80                     0.0        1981         6.1      -0.930851   \n",
      "81                     0.0        1981         6.6      -0.944369   \n",
      "82                     2.0        1982         5.8       0.749428   \n",
      "83                     2.0        1982         8.2       0.399855   \n",
      "84                     1.0        1982         7.4       0.100220   \n",
      "85                     1.0        1982         5.9       0.050281   \n",
      "86                     0.0        1982         6.9       0.000342   \n",
      "87                     0.0        1982         6.9       0.000342   \n",
      "88                     2.0        1982         3.5       0.000342   \n",
      "89                     0.0        1982         7.8      -0.199415   \n",
      "90                     0.0        1982         8.2      -0.249354   \n",
      "91                     2.0        1982         7.7      -0.449111   \n",
      "92                     0.0        1982         7.4      -0.464092   \n",
      "93                     0.0        1982         7.4      -0.464092   \n",
      "94                     0.0        1982         7.9      -0.474080   \n",
      "95                     6.0        1982         7.2      -0.748745   \n",
      "96                     0.0        1982         5.7      -0.798685   \n",
      "97                     0.0        1982         4.6      -0.873593   \n",
      "98                     0.0        1983         8.4       0.566840   \n",
      "99                     0.0        1983         7.9       0.301946   \n",
      "...                    ...         ...         ...            ...   \n",
      "3027                   4.0        2015         6.1      -0.376442   \n",
      "3028                   1.0        2015         4.4      -0.396507   \n",
      "3029                   0.0        2015         7.5      -0.396507   \n",
      "3030                   2.0        2015         7.6      -0.396507   \n",
      "3031                   8.0        2015         6.5      -0.416571   \n",
      "3032                   0.0        2015         7.9      -0.436635   \n",
      "3033                   4.0        2015         7.8      -0.436635   \n",
      "3034                   2.0        2015         6.1      -0.436635   \n",
      "3035                   0.0        2015         6.5      -0.476764   \n",
      "3036                   1.0        2015         7.2      -0.496829   \n",
      "3037                   2.0        2015         8.1      -0.597151   \n",
      "3038                   0.0        2015         6.8      -0.597151   \n",
      "3039                   1.0        2015         6.6      -0.597151   \n",
      "3040                   4.0        2015         7.0      -0.597151   \n",
      "3041                   0.0        2015         7.4      -0.657345   \n",
      "3042                  13.0        2015         5.7      -0.657345   \n",
      "3043                   0.0        2015         6.2      -0.697474   \n",
      "3044                   3.0        2015         5.7      -0.701487   \n",
      "3045                   4.0        2015         5.1      -0.717538   \n",
      "3046                   0.0        2015         4.5      -0.727570   \n",
      "3047                   2.0        2015         6.4      -0.757667   \n",
      "3048                   0.0        2015         6.4      -0.757667   \n",
      "3049                   1.0        2015         7.5      -0.757667   \n",
      "3050                   0.0        2015         5.2      -0.797796   \n",
      "3051                   0.0        2015         7.1      -0.797796   \n",
      "3052                   0.0        2015         5.3      -0.797796   \n",
      "3053                   4.0        2015         6.5      -0.827893   \n",
      "3054                   2.0        2015         5.6      -0.827893   \n",
      "3055                   0.0        2015         4.5      -0.827993   \n",
      "3056                   0.0        2015         7.3      -0.837925   \n",
      "3057                   3.0        2015         7.3      -0.857990   \n",
      "3058                   0.0        2015         6.5      -0.857990   \n",
      "3059                   0.0        2015         6.2      -0.898119   \n",
      "3060                   1.0        2015         7.1      -0.898119   \n",
      "3061                   1.0        2015         7.1      -0.898119   \n",
      "3062                   0.0        2015         6.8      -0.898119   \n",
      "3063                   0.0        2015         4.6      -0.898119   \n",
      "3064                   0.0        2015         7.1      -0.898119   \n",
      "3065                   1.0        2015         4.6      -0.918183   \n",
      "3066                   0.0        2015         6.8      -0.928215   \n",
      "3067                   0.0        2015         5.2      -0.932228   \n",
      "3068                   5.0        2015         5.9      -0.952293   \n",
      "3069                   0.0        2015         7.2      -0.956306   \n",
      "3070                   1.0        2015         5.3      -0.958312   \n",
      "3071                   3.0        2015         6.9      -0.958312   \n",
      "3072                   6.0        2015         7.2      -0.960319   \n",
      "3073                   1.0        2015         5.5      -0.978377   \n",
      "3074                   0.0        2015         4.2      -0.996435   \n",
      "3075                   0.0        2016         8.2       3.950090   \n",
      "3076                   0.0        2016         6.9       3.950090   \n",
      "3077                   4.0        2016         7.5       2.663472   \n",
      "3078                   2.0        2016         6.6       2.564501   \n",
      "3079                   2.0        2016         6.6       2.564501   \n",
      "3080                   6.0        2016         7.3       2.524913   \n",
      "3081                   8.0        2016         6.9       2.465531   \n",
      "3082                   1.0        2016         6.4       2.366560   \n",
      "3083                   0.0        2016         5.5       2.267590   \n",
      "3084                   0.0        2016         7.3       2.168619   \n",
      "3085                   0.0        2016         7.2       1.871707   \n",
      "3086                   4.0        2016         5.5       1.851913   \n",
      "3087                   4.0        2016         5.5       1.851913   \n",
      "3088                   4.0        2016         5.5       1.772736   \n",
      "3089                   0.0        2016         6.3       1.673766   \n",
      "3090                   2.0        2016         6.1       1.277883   \n",
      "3091                   3.0        2016         5.8       1.178913   \n",
      "3092                   6.0        2016         6.9       0.783030   \n",
      "3093                   0.0        2016         6.3       0.446530   \n",
      "3094                   0.0        2016         8.1       0.149618   \n",
      "3095                   2.0        2016         6.6      -0.008735   \n",
      "3096                   0.0        2016         7.4      -0.008735   \n",
      "3097                   4.0        2016         4.8      -0.008735   \n",
      "3098                   1.0        2016         6.7      -0.008735   \n",
      "3099                   0.0        2016         7.8      -0.206676   \n",
      "3100                   0.0        2016         5.9      -0.206676   \n",
      "3101                   4.0        2016         6.1      -0.246264   \n",
      "3102                   0.0        2016         5.2      -0.246264   \n",
      "3103                   0.0        2016         6.0      -0.305647   \n",
      "3104                   1.0        2016         5.3      -0.424411   \n",
      "3105                   0.0        2016         5.8      -0.444206   \n",
      "3106                   1.0        2016         6.7      -0.464000   \n",
      "3107                   0.0        2016         5.8      -0.503588   \n",
      "3108                   9.0        2016         6.7      -0.602559   \n",
      "3109                   9.0        2016         6.7      -0.602559   \n",
      "3110                   2.0        2016         6.3      -0.602559   \n",
      "3111                   0.0        2016         7.1      -0.602559   \n",
      "3112                   3.0        2016         6.3      -0.602559   \n",
      "3113                   0.0        2016         5.4      -0.632250   \n",
      "3114                  10.0        2016         6.1      -0.642147   \n",
      "3115                   0.0        2016         6.8      -0.661941   \n",
      "3116                   0.0        2016         7.3      -0.701529   \n",
      "3117                   2.0        2016         6.4      -0.701529   \n",
      "3118                   0.0        2016         6.8      -0.741117   \n",
      "3119                   1.0        2016         6.0      -0.770809   \n",
      "3120                   0.0        2016         6.0      -0.800500   \n",
      "3121                   1.0        2016         4.8      -0.800500   \n",
      "3122                  11.0        2016         3.4      -0.899470   \n",
      "3123                   4.0        2016         3.5      -0.899470   \n",
      "3124                   6.0        2016         4.5      -0.899470   \n",
      "3125                   0.0        2016         6.9      -0.901450   \n",
      "3126                   0.0        2016         6.8      -0.497794   \n",
      "\n",
      "      scaled_critics  scaled_duration  scaled_voted_users  scaled_user_reviews  \n",
      "0          -0.586886        -0.792112           -0.555342            -0.380577  \n",
      "1          -0.980827        -0.333208           -0.657406            -0.677521  \n",
      "2          -1.141619        -0.103756           -0.657987            -0.589174  \n",
      "3          -0.956708         1.961311           -0.617814            -0.547455  \n",
      "4          -0.972787         1.181175           -0.637597            -0.640710  \n",
      "5          -0.771797        -1.342796           -0.647479            -0.594083  \n",
      "6          -0.233143        -0.057866           -0.033753            -0.120444  \n",
      "7           0.144718         0.492819            0.440898             0.048888  \n",
      "8           1.021035        -0.057866            2.020190             1.742207  \n",
      "9          -0.345698         1.961311           -0.218814            -0.034551  \n",
      "10         -0.820035         4.026378           -0.490908             0.034163  \n",
      "11         -0.651203         2.787338           -0.250497            -0.176888  \n",
      "12         -0.144708         1.364736            0.007882            -0.174434  \n",
      "13         -1.117500        -0.654440           -0.672526            -0.746236  \n",
      "14         -1.093381         5.311308           -0.636799            -0.564634  \n",
      "15         -0.594926         4.164049           -0.321676            -0.184250  \n",
      "16         -0.353738         2.970899            0.268274             0.186317  \n",
      "17         -0.884351         1.961311           -0.644400            -0.645618  \n",
      "18         -0.265302         0.033915            0.293295            -0.051730  \n",
      "19         -1.133579         1.548298           -0.668335            -0.687338  \n",
      "20         -0.498450        -0.011975           -0.629772            -0.473832  \n",
      "21         -0.884351         4.852405           -0.597533            -0.655435  \n",
      "22         -0.779837         3.292132           -0.487612            -0.441929  \n",
      "23         -1.004946        -0.562660           -0.656882            -0.687338  \n",
      "24         -1.246134        -2.031152           -0.673088            -0.770777  \n",
      "25          0.361787         3.016790            6.704599             4.682202  \n",
      "26         -0.723559        -0.057866           -0.570954            -0.360944  \n",
      "27          1.133590         1.043503            1.137525             1.786381  \n",
      "28         -0.353738         0.905832            0.443523            -0.191613  \n",
      "29         -0.506490         0.125696           -0.270427            -0.225970  \n",
      "30         -0.410015         0.125696           -0.245144            -0.262781  \n",
      "31         -0.112549         5.081857            4.374052             0.785114  \n",
      "32         -0.273341        -0.149646            0.041501            -0.037005  \n",
      "33         -0.337658        -0.746221           -0.069499             0.080791  \n",
      "34         -1.270253        -1.067454           -0.656429            -0.721695  \n",
      "35          0.916520        -0.975673           -0.062371             1.217033  \n",
      "36          0.916520        -0.975673           -0.062364             1.217033  \n",
      "37          1.784798         0.951723            1.956452             1.550788  \n",
      "38         -0.112549         1.089394            3.665742             1.055063  \n",
      "39         -0.410015         0.446929           -0.423683            -0.216153  \n",
      "40         -0.176866         1.640078            1.718737             0.520072  \n",
      "41         -0.860233         3.016790           -0.420936            -0.294684  \n",
      "42          0.064322         1.181175            0.211525             0.021893  \n",
      "43          0.956718         0.722271            5.141679             2.797464  \n",
      "44         -0.072351        -0.746221            0.554243             0.394914  \n",
      "45         -0.546688        -0.562660           -0.657304            -0.603899  \n",
      "46          0.048243         3.613364            0.128925             0.409638  \n",
      "47         -1.053183         0.401038           -0.608622            -0.441929  \n",
      "48         -0.707480         0.997613           -0.350434            -0.159709  \n",
      "49         -0.313539         0.033915            0.407879             0.048888  \n",
      "50         -0.554728        -0.011975           -0.102186            -0.179342  \n",
      "51          1.246144        -0.379098            0.330140             2.112774  \n",
      "52          1.246144        -0.379098            0.330140             2.112774  \n",
      "53          1.246144        -0.379098            0.330178             2.112774  \n",
      "54         -0.739639         0.355148           -0.597661            -0.532730  \n",
      "55         -0.233143         1.548298           -0.273679             0.183863  \n",
      "56          0.787887         8.248292            2.200606             1.602324  \n",
      "57         -1.053183         0.722271           -0.638984            -0.682430  \n",
      "58         -0.884351         0.125696           -0.121905            -0.461562  \n",
      "59         -0.337658         0.309257           -0.190631            -0.149893  \n",
      "60         -0.490411         9.900346           -0.615425            -0.346220  \n",
      "61         -0.305500         1.777749            0.231711            -0.027189  \n",
      "62         -1.101421         0.676380           -0.659067            -0.628440  \n",
      "63          0.482381         0.814052            4.673212             1.398635  \n",
      "64         -0.096470         0.538709            0.823763             0.402276  \n",
      "65         -1.109460        -0.011975           -0.562343            -0.662797  \n",
      "66         -0.337658        -0.241427           -0.525000            -0.316771  \n",
      "67         -0.811995         0.676380           -0.453814            -0.115536  \n",
      "68         -0.739639        -0.516769           -0.156980            -0.196521  \n",
      "69         -0.233143        -0.975673            0.345624             0.237852  \n",
      "70          0.393946        -0.929783           -0.381236             0.012077  \n",
      "71          0.393946        -0.929783           -0.381230             0.012077  \n",
      "72         -0.940629         0.676380           -0.595342            -0.603899  \n",
      "73         -1.205936        -0.516769           -0.665052            -0.726603  \n",
      "74          0.570817         0.263367            3.544221             1.082058  \n",
      "75         -1.109460        -0.516769           -0.669274            -0.709425  \n",
      "76         -0.828074        -0.057866           -0.609025            -0.559725  \n",
      "77         -0.908470        -0.333208           -0.670073            -0.743782  \n",
      "78         -1.294371         0.401038           -0.672519            -0.758506  \n",
      "79         -0.892391         0.355148           -0.367586            -0.481195  \n",
      "80          0.635134        -1.021564           -0.424686             0.156868  \n",
      "81         -0.016074        -0.838002           -0.532748            -0.331495  \n",
      "82         -1.222015         0.217477           -0.628725            -0.692246  \n",
      "83          1.117510         0.355148            2.270444             2.056330  \n",
      "84         -0.554728         0.309257           -0.184020            -0.333949  \n",
      "85         -0.964748         1.227065           -0.551535            -0.569542  \n",
      "86          0.024124         0.905832            0.044018             0.016985  \n",
      "87          0.024124         0.905832            0.044037             0.016985  \n",
      "88         -0.988866        -0.470879           -0.661986            -0.677521  \n",
      "89         -0.675322         0.905832           -0.510155            -0.432113  \n",
      "90          1.077312        -0.011975            0.970331             1.425630  \n",
      "91         -0.120589         0.309257           -0.094284             0.070975  \n",
      "92          0.482381         0.492819           -0.004650            -0.022281  \n",
      "93          0.482381         0.492819           -0.004638            -0.022281  \n",
      "94          0.418065         0.492819            1.122130             0.453812  \n",
      "95         -0.972787         0.033915           -0.582432            -0.596537  \n",
      "96          0.578857        -0.838002           -0.474115             0.102878  \n",
      "97         -0.008034        -0.516769           -0.495597             0.495531  \n",
      "98          0.273352         1.135284            3.677342             0.777751  \n",
      "99         -0.667282         3.842816           -0.389036            -0.392848  \n",
      "...              ...              ...                 ...                  ...  \n",
      "3027        0.329629        -0.470879           -0.255838            -0.272597  \n",
      "3028       -0.699441        -0.700331           -0.528277            -0.508190  \n",
      "3029        1.141629         0.676380            0.261605            -0.130260  \n",
      "3030        2.532481         0.538709            0.575176             0.321291  \n",
      "3031        0.466302         0.263367           -0.054149            -0.356036  \n",
      "3032        1.495372         2.649667            0.087858             0.002260  \n",
      "3033        2.114421         0.951723            0.490640             0.107786  \n",
      "3034       -0.305500        -0.103756           -0.604911            -0.687338  \n",
      "3035        0.120599         0.355148           -0.309808            -0.500827  \n",
      "3036        0.410025         0.125696           -0.079253            -0.297138  \n",
      "3037        2.500323         0.859942            0.569529             0.193679  \n",
      "3038       -0.820035         0.079806           -0.557392            -0.596537  \n",
      "3039        0.096480        -0.379098           -0.286263            -0.493465  \n",
      "3040       -1.093381         0.401038           -0.663513            -0.760960  \n",
      "3041       -0.401975         0.905832           -0.528232            -0.606353  \n",
      "3042       -0.530609        -0.103756           -0.620402            -0.643164  \n",
      "3043        0.281391        -0.516769           -0.487433            -0.365853  \n",
      "3044        0.474342         0.263367           -0.434191            -0.478741  \n",
      "3045       -0.450213        -0.470879           -0.498344            -0.574450  \n",
      "3046       -0.715520         0.905832           -0.666988            -0.736420  \n",
      "3047        0.225114        -0.011975           -0.273295            -0.417388  \n",
      "3048        0.112560        -0.149646           -0.311884            -0.375669  \n",
      "3049       -1.302411         0.079806           -0.676212            -0.785501  \n",
      "3050        0.209035        -0.562660           -0.517181            -0.500827  \n",
      "3051       -0.192945        -0.149646           -0.545287            -0.564634  \n",
      "3052       -0.257262         0.584600           -0.627268            -0.660343  \n",
      "3053       -0.249223        -0.379098           -0.350358            -0.525368  \n",
      "3054        0.747688        -0.654440           -0.485185            -0.319225  \n",
      "3055       -0.562767        -0.838002           -0.632232            -0.679976  \n",
      "3056        1.326540        -0.883892           -0.477072            -0.466470  \n",
      "3057        0.136678        -0.287317           -0.316637            -0.591628  \n",
      "3058       -0.241183        -0.287317           -0.640766            -0.748690  \n",
      "3059        1.672243        -0.700331           -0.261101             0.306567  \n",
      "3060        1.077312        -0.057866           -0.167775            -0.125352  \n",
      "3061        1.077312        -0.057866           -0.167731            -0.125352  \n",
      "3062        0.321589        -0.287317           -0.367292            -0.284868  \n",
      "3063       -1.213975         0.538709           -0.665142            -0.738874  \n",
      "3064        1.278302        -0.654440           -0.493962            -0.503281  \n",
      "3065       -0.112549        -0.838002           -0.504482            -0.488557  \n",
      "3066        2.106382        -0.792112           -0.253538             0.299205  \n",
      "3067       -0.088431        -1.205125           -0.496082            -0.500827  \n",
      "3068       -1.222015         0.492819           -0.646425            -0.655435  \n",
      "3069       -1.278292         0.722271           -0.678077            -0.807588  \n",
      "3070       -0.996906        -0.562660           -0.653235            -0.756052  \n",
      "3071        0.040203        -0.333208           -0.571893            -0.633348  \n",
      "3072       -1.230055        -0.929783           -0.673465            -0.763414  \n",
      "3073       -1.021025        -0.562660           -0.671523            -0.792864  \n",
      "3074       -0.032153        -1.296906           -0.591848            -0.441929  \n",
      "3075        2.837986         1.731859            1.063541             1.698033  \n",
      "3076        4.100204         3.383913            1.695735             6.596388  \n",
      "3077        1.278302         0.584600           -0.335787             0.250123  \n",
      "3078        0.683372         0.033915           -0.407554            -0.223516  \n",
      "3079        0.683372         0.033915           -0.407554            -0.223516  \n",
      "3080        1.873233         1.594188            0.269597             0.716399  \n",
      "3081        2.050105         0.630490            0.081879             1.572875  \n",
      "3082        0.442183         0.171586           -0.541825            -0.488557  \n",
      "3083        0.988877         0.492819           -0.306850             0.466082  \n",
      "3084        0.900441         0.630490            0.034718             1.106599  \n",
      "3085        0.377867        -0.654440           -0.267342            -0.454200  \n",
      "3086        2.419926         0.309257           -0.232624             2.161856  \n",
      "3087        2.419926         0.309257           -0.232414             2.161856  \n",
      "3088        0.168837         0.768161           -0.346742            -0.140077  \n",
      "3089        0.144718         0.125696           -0.566220            -0.527822  \n",
      "3090        0.546698         0.492819           -0.437078            -0.481195  \n",
      "3091        0.144718         0.492819           -0.395264            -0.456654  \n",
      "3092        0.265312         0.905832           -0.417200            -0.468924  \n",
      "3093       -0.176866        -0.562660           -0.504917            -0.500827  \n",
      "3094        3.344481        -0.057866            2.381834             1.786381  \n",
      "3095        0.112560        -0.103756           -0.465159            -0.540093  \n",
      "3096        0.329629         1.594188           -0.373111            -0.272597  \n",
      "3097        0.506500        -0.333208           -0.454875            -0.441929  \n",
      "3098       -0.675322         1.364736           -0.658562            -0.694700  \n",
      "3099        1.101431         1.135284           -0.263081            -0.125352  \n",
      "3100       -0.353738        -0.333208           -0.495392            -0.667705  \n",
      "3101       -0.120589         0.033915           -0.426283            -0.606353  \n",
      "3102        0.249233         0.125696           -0.322948            -0.157255  \n",
      "3103        0.112560        -0.792112           -0.499097            -0.537639  \n",
      "3104       -0.072351        -0.470879           -0.569727            -0.574450  \n",
      "3105        0.498461        -0.057866           -0.526348            -0.481195  \n",
      "3106        0.844164        -0.516769           -0.552946            -0.557271  \n",
      "3107       -0.321579        -0.516769           -0.621462            -0.672613  \n",
      "3108       -0.659243        -0.424989           -0.648489            -0.697154  \n",
      "3109       -0.659243        -0.424989           -0.648489            -0.697154  \n",
      "3110       -0.329619        -0.103756           -0.599801            -0.522914  \n",
      "3111       -0.619045        -0.608550           -0.650731            -0.724149  \n",
      "3112        0.410025         0.263367           -0.470186            -0.549909  \n",
      "3113       -1.117500         0.079806           -0.668961            -0.736420  \n",
      "3114       -0.056272        -0.700331           -0.591586            -0.557271  \n",
      "3115        0.184916        -1.067454           -0.595285            -0.468924  \n",
      "3116        1.993827        -0.241427            0.132348             0.269756  \n",
      "3117       -0.273341        -0.424989           -0.579941            -0.603899  \n",
      "3118       -0.803955        -0.011975           -0.638128            -0.675067  \n",
      "3119       -0.040193        -0.011975           -0.360929            -0.402664  \n",
      "3120       -0.032153        -0.562660           -0.450467            -0.429659  \n",
      "3121        0.176876        -0.746221           -0.545115            -0.498373  \n",
      "3122       -1.077302         0.492819           -0.649466            -0.559725  \n",
      "3123       -0.836114        -0.792112           -0.617476            -0.679976  \n",
      "3124       -1.213975        -0.608550           -0.670680            -0.787955  \n",
      "3125       -0.032153        -1.296906           -0.591835            -0.576904  \n",
      "3126        0.120599         0.355148           -0.502675            -0.532730  \n",
      "\n",
      "[3127 rows x 8 columns]\n",
      "<generator object _BaseKFold.split at 0x000001DF32A17A40>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "drop_list = ['scaled_gross', 'num_critic_for_reviews', 'encoded_gross', 'gross_group', \n",
    "              'gross', 'budget', 'director_name', 'actor_1_name', 'content_rating', 'duration',\n",
    "            'num_voted_users', 'num_user_for_reviews']\n",
    "X = df.drop(drop_list, axis=1)\n",
    "y = df['encoded_gross'].values # x and y are now np.matrices \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cpnvert dataframe to np matrix\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### [20 points] Custom MLP\n",
    " Create a custom implementation of the multi-layer perceptron. Start with the implementation given to you in the course. Update the MLP class to:\n",
    "- When instantiated, use a selectable nonlinearity function for the first layer: either sigmoid or linear \n",
    "- Use a selectable cost function when instantiated: either quadratic or cross entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Tune hyper-parameters, visualize eval metrics with diff hyper-parameters\n",
    "Tune the hyper-parameters of your MLP model (at least two hyper-parameters). While tuning hyper-parameters, analyze the results using your chosen metric(s) of evaluation. Visualize the evaluation metric(s) versus the hyper-parameters. Conclude what combination of parameters are best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Compare custom MLP to scikit-learn\n",
    "Compare the performance of your MLP training procedure to scikit-learn. Which implementation is better in terms of generalization performance, computation time, and memory used while training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "One idea: Add support for more than one hidden layer or a different nonlinear activation (i.e., softmax, tanh). Please note that different activation functions might require different weight initializations to work well. \n",
    "\n",
    "- Hidden layer: input - hidden - output layer. You get to select how many neurons are in the hidden layer. Evrything before the output layer is a hidden layer. Output layer is set, because its the number of classes we want to predict. That means there are 4 neourons at the output layer for us since w ehave 4 classes.\n",
    "\n",
    "- Dead neouron: one that never gets updated in the gradient. Like when the sigmoid gradient is always at 0 (if you've initiliazed very poorly). Relu nonlinearity can give you some dead neurons but not as many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
